{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import io\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.subplots\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in file\n",
    "parts = pd.read_csv(\"/Users/SDevitt/OneDrive - Sense Corp/Jupyter_notebooks/kobayashi_maru_december2019.csv\")\n",
    "parts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic data summary, counts, looking for null etc\n",
    "print (\"Rows     : \" ,parts.shape[0])\n",
    "print (\"Columns  : \" ,parts.shape[1])\n",
    "print (\"\\nFeatures : \\n\" ,parts.columns.tolist())\n",
    "print (\"\\nMissing values :  \", parts.isnull().sum().values.sum())\n",
    "print (\"\\nUnique values :  \\n\",parts.nunique())\n",
    "print(parts.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot frequency of each label\n",
    "plot = plt.figure(figsize=(8,6))\n",
    "parts.groupby('Labels').Text.count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no missing data, 155 unique labels, 10067 rows, 10067 unique inventory items, 4 cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see our data is quite imbalanced with 25% of the data from a single category. Lets see what that category is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts['Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So about a quarter of the dataset are misc. I think this may need to be a 2 stage classification (misc vs everything else and the a model for the remaining 154 classes), but will leave this for now and see how the model does. Another note, there are many classes with just one record in the training set. These are important, I will see later if I need to do anything to make the dataset more balanced.\n",
    "\n",
    "Inv Item      10067\n",
    "Labels          155\n",
    "Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "#add a numerical id for each label\n",
    "parts['Label_id']=parts['Labels'].factorize()[0]\n",
    "parts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary w labels & label_id, I will use this later\n",
    "df = parts[['Labels','Label_id']].drop_duplicates().sort_values('Label_id')\n",
    "\n",
    "label_to_id = dict(df.values)\n",
    "#print(label_to_id)\n",
    "id_to_label = dict(df[['Label_id','Labels']].values)\n",
    "#print(id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "cols = ['Text', 'Inv Item']\n",
    "\n",
    "#define X & Y\n",
    "X = parts[cols]\n",
    "Y = parts['Label_id']\n",
    "\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "ros_X, ros_Y = ros.fit_resample(X, Y)\n",
    "\n",
    "# using Counter to display results of naive oversampling\n",
    "from collections import Counter\n",
    "print(sorted(Counter(ros_Y).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now each category is represented as many times as misc - 2795 times; note I tried running this after the TFIDF vectorization and couldn't ever get to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a new data frame with our vectorized data and the Label_id\n",
    "dfY = pd.DataFrame(ros_Y)\n",
    "dfX = pd.DataFrame(ros_X, columns=cols)\n",
    "df = dfY.merge(dfX,left_index=True,right_index=True,how = \"left\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now the table has 433224 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets use tf idf to vectorize our text col.\n",
    "This is as far as I get. I can get this code to run fine on the original datatable but not on the RoS one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=10, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "features = tfidf.fit_transform(df.Text).toarray()\n",
    "labels = df.Label_id\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our text field was vectorized into 40737 features. We can use chi2 to see which features are most correlated with each class. Note, this runs fine on non resampled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "N = 2\n",
    "for Labels, Label_id in sorted(label_to_id.items()):\n",
    "  features_chi2 = chi2(features, labels == Label_id)\n",
    "  indices = np.argsort(features_chi2[0])\n",
    "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "  print(\"# '{}':\".format(Labels))\n",
    "  print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
    "  print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a new data frame with our vectorized data and the Label_id\n",
    "#this runs fine on non resampled dataset\n",
    "df = pd.DataFrame(features)\n",
    "parts_og = parts.copy()\n",
    "df = parts.merge(df,left_index=True,right_index=True,how = \"left\")\n",
    "df.iloc[:10,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this runs on non resampled data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train, X_test, y_train, y_test = train_test_split(parts['Text'], parts['Labels'], random_state = 0)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "import seaborn as sns\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
